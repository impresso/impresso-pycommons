{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rebuilt Manifest Recomputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import git\n",
    "from dask import bag as db\n",
    "from dask import dataframe as dd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from time import strftime\n",
    "import copy\n",
    "from typing import Any\n",
    "\n",
    "from impresso_commons.utils.s3 import (fixed_s3fs_glob, alternative_read_text, upload,\n",
    "                                       get_storage_options, get_boto3_bucket, IMPRESSO_STORAGEOPT)\n",
    "from impresso_commons.versioning.helpers import (DataStage, read_manifest_from_s3, \n",
    "                                                 validate_stage, clone_git_repo,\n",
    "                                                 write_and_push_to_git, write_dump_to_fs)\n",
    "from impresso_commons.path import parse_canonical_filename\n",
    "from impresso_commons.path.path_fs import IssueDir\n",
    "from impresso_commons.path.path_s3 import read_s3_issues, list_newspapers\n",
    "from impresso_commons.versioning.data_statistics import NewspaperStatistics, POSSIBLE_GRANULARITIES\n",
    "from impresso_commons.versioning.data_manifest import DataManifest\n",
    "from impresso_commons.versioning.helpers import counts_for_rebuilt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rebuilt_stats_for_manifest(\n",
    "    s3_rebuilt_files: db.core.Bag,\n",
    ") -> list[dict[str, Any]]:\n",
    "\n",
    "    # all the rebuilt articles in the bag are form the same newspaper and year\n",
    "    # define locally the nunique() aggregation function for dask\n",
    "    def chunk(s):\n",
    "        # The function applied to the individual partition (map)\n",
    "        return s.apply(lambda x: list(set(x)))\n",
    "\n",
    "    def agg(s):\n",
    "        # The function which will aggregate the result from all the partitions (reduce)\n",
    "        s = s._selected_obj\n",
    "        return s.groupby(level=list(range(s.index.nlevels))).sum()\n",
    "\n",
    "    def finalize(s):\n",
    "        # The optional function that will be applied to the result of the agg_tu functions\n",
    "        return s.apply(lambda x: len(set(x)))\n",
    "\n",
    "    # aggregating function implementing np.nunique()\n",
    "    tunique = dd.Aggregation(\"tunique\", chunk, agg, finalize)\n",
    "\n",
    "    # for each rebuilt content-item, fetch the information necessary to compute the statistics we seek\n",
    "    rebuilt_count_df = (\n",
    "        s3_rebuilt_files.map(lambda rf: counts_for_rebuilt(rf, include_np=True))\n",
    "        .to_dataframe(meta={'np_id': str, 'year': str, 'issue_id': str,\n",
    "                            'n_content_items': int, 'n_tokens': int})\n",
    "        .persist()\n",
    "    )\n",
    "\n",
    "    # agggregate them at the scale of the entire corpus\n",
    "    # first groupby title, year and issue to also count the individual issues present\n",
    "    aggregated_df = (rebuilt_count_df\n",
    "            #.groupby(by=['np_id', 'year', 'issue_id']) \n",
    "            #.agg({'n_content_items': sum, 'n_tokens': sum})\n",
    "            #.reset_index()\n",
    "            .groupby(by=['np_id', 'year']) \n",
    "            .agg({\"issue_id\": tunique, 'n_content_items': sum, 'n_tokens': sum})\n",
    "            .rename(columns={'issue_id': 'issues', 'n_content_items': 'content_items_out', 'n_tokens': 'ft_tokens'})\n",
    "            .reset_index()\n",
    "    )\n",
    "\n",
    "    print(\"Obtaining the yearly rebuilt statistics for the entire corpus\")\n",
    "    # return as a list of dicts\n",
    "    return aggregated_df.to_bag(format='dict').compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rebuilt manifest of Run 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information to initialize the Manifest object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_rebuilt_bucket = \"rebuilt-staging\"\n",
    "#manifest_out_name = 'rebuilt_v0-0-1.json'\n",
    "\n",
    "pycommons_repo = git.Repo('/home/piconti/impresso-pycommons')\n",
    "mft_s3_input_bucket = 'canonical-staging' # bucket corresponding to the input data of the data currently in 'rebuilt-data'\n",
    "mft_s3_output_bucket = 'rebuilt-staging' #'rebuilt-data' #'rebuilt-sandbox'\n",
    "# there is no previous manifest\n",
    "temp_dir = '/home/piconti/temp_rebuilt_v0-2-0'\n",
    "\n",
    "rebuilt_mft_run_2 = DataManifest(\n",
    "    data_stage = 'rebuilt', # DataStage.REBUILT also works\n",
    "    s3_output_bucket = mft_s3_output_bucket,\n",
    "    s3_input_bucket = mft_s3_input_bucket,\n",
    "    git_repo = pycommons_repo,\n",
    "    temp_dir = temp_dir,\n",
    "    staging=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newspapers = [\n",
    "    'arbeitgeber', 'handelsztg', 'actionfem', 'deletz1893', 'demitock', 'diekwochen', \n",
    "    'dunioun', 'indeplux', 'kommmit', 'lunion', 'luxembourg1935', 'luxland'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the rebuilt files\n",
    "rebuilt_files_2 = []\n",
    "\n",
    "for np in newspapers:\n",
    "    rebuilt_files_2.extend(fixed_s3fs_glob(os.path.join(s3_rebuilt_bucket, np, '*.jsonl.bz2')))\n",
    "\n",
    "rebuilt_bag = db.read_text(rebuilt_files_2, storage_options=IMPRESSO_STORAGEOPT).map(json.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_2_stats = compute_rebuilt_stats_for_manifest(rebuilt_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebuilt_stats = copy.deepcopy(run_2_stats)\n",
    "\n",
    "print(\"Populating the manifest with the resulting yearly statistics...\")\n",
    "# populate the manifest with these statistics\n",
    "for stats in rebuilt_stats:\n",
    "    title = stats['np_id']\n",
    "    year = stats['year']\n",
    "    del stats[\"np_id\"]\n",
    "    del stats[\"year\"]\n",
    "    rebuilt_mft_run_2.add_by_title_year(title, year, stats)\n",
    "\n",
    "print(\"Finalizing the manifest, and computing the result...\")\n",
    "\n",
    "note = f\"Rebuilt of newspaper articles for {newspapers}.\"\n",
    "rebuilt_mft_run_2.append_to_notes(note)\n",
    "rebuilt_mft_run_2.compute(export_to_git_and_s3 = True)\n",
    "#rebuilt_mft_run_2.validate_and_export_manifest(path_to_schema=schema_path, push_to_git=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebuilt_mft_run_2.manifest_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the statistics based on the current rebuilt data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Fetching the rebuilt files from the bucket s3://{s3_rebuilt_bucket}\")\n",
    "\n",
    "rebuilt_files = fixed_s3fs_glob(os.path.join(s3_rebuilt_bucket, '*.jsonl.bz2'))\n",
    "\n",
    "print(\"Reading the contents of the fetched files...\")\n",
    "# lazy object, to comput once reduced\n",
    "rebuilt_bag = db.read_text(rebuilt_files, storage_options=IMPRESSO_STORAGEOPT).map(json.loads)\n",
    "\n",
    "# for each rebuilt content-item, fetch the information necessary to compute the statistics we seek\n",
    "rebuilt_count_df = (\n",
    "    rebuilt_bag.map(\n",
    "        lambda ci: {\n",
    "            \"np_id\": ci[\"id\"].split('-')[0], \n",
    "            \"year\": ci[\"id\"].split('-')[1], \n",
    "            \"issue_id\": '-'.join(ci['id'].split('-')[:-1]), # count the issues represented\n",
    "            \"n_content_items\": 1, # each element of the bag corresponds to one content-item\n",
    "            \"n_tokens\": len(ci['ft'].split()) if 'ft' in ci else 0 # split on spaces to count tokens\n",
    "        }\n",
    "    )\n",
    "    .to_dataframe(meta={'np_id': str, 'year': str, 'issue_id': str,\n",
    "                        'n_content_items': int, 'n_tokens': int})\n",
    "    .persist()\n",
    ")\n",
    "\n",
    "# agggregate them at the scale of the entire corpus\n",
    "# first groupby title, year and issue to also count the individual issues present\n",
    "aggregated_df = (rebuilt_count_df\n",
    "        .groupby(by=['np_id', 'year', 'issue_id']) \n",
    "        .agg({'n_content_items': sum, 'n_tokens': sum})\n",
    "        .reset_index()\n",
    "        .groupby(by=['np_id', 'year']) \n",
    "        .agg({'issue_id': 'count', 'n_content_items': sum, 'n_tokens': sum})\n",
    "        .rename(columns={'issue_id': 'issues', 'n_content_items': 'content_items_out', 'n_tokens': 'ft_tokens'})\n",
    "        .reset_index()\n",
    ")\n",
    "\n",
    "print(\"Obtaining the yearly rebuilt statistics for the entire corpus\")\n",
    "# return as a list of dicts\n",
    "stats_as_dict = aggregated_df.to_bag(format='dict').compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebuilt_stats = copy.deepcopy(stats_as_dict)\n",
    "\n",
    "print(\"Populating the manifest with the resulting yearly statistics...\")\n",
    "# populate the manifest with these statistics\n",
    "for stats in rebuilt_stats:\n",
    "    title = stats['np_id']\n",
    "    year = stats['year']\n",
    "    del stats[\"np_id\"]\n",
    "    del stats[\"year\"]\n",
    "    rebuilt_mft_0.add_by_title_year(title, year, stats)\n",
    "\n",
    "print(\"Finalizing the manifest, and computing the result...\")\n",
    "\n",
    "note = f\"Initial Manifest computed retroactively on the rebuilt data present in the bucket s3://{s3_rebuilt_bucket}.\"\n",
    "rebuilt_mft_0.append_to_notes(note)\n",
    "rebuilt_mft_0.append_to_notes(\"test\", False)\n",
    "rebuilt_mft_0.compute(export_to_git_and_s3 = False)\n",
    "rebuilt_mft_0.validate_and_export_manifest(path_to_schema=schema_path, push_to_git=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patches",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
